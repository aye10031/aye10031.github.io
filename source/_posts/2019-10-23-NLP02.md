---


layout: post

title: "CS 224N学习笔记"

date: 2019-10-23

tag: 学习
typora-root-url: ..
---



# Day2 词向量表示



[视频](https://www.bilibili.com/video/av41393758)





## 词义

nltk安装：

首先需要下载相关包，运行以下代码：

```python
import nltk

nltk.download()
```

运行之后会跳出如下界面：

![正常下载界面](/images/posts/nlp02/nltkdl.png)

如果点击all的话可能要下载很久，因此最好是需要什么下什么：

![](/images/posts/nlp02/nltkdl2.png)

这里仅仅下载了wordnet这一包

```python
from nltk.corpus import wordnet as wn

panda = wn.synset('panda.n.01')
hyper = lambda s: s.hypernyms()

list = list(panda.closure(hyper))

for i in list:
    print(i)
    
>>>Synset('procyonid.n.01')
>>>Synset('carnivore.n.01')
>>>Synset('placental.n.01')
>>>Synset('mammal.n.01')
>>>Synset('vertebrate.n.01')
>>>Synset('chordate.n.01')
>>>Synset('animal.n.01')
>>>Synset('organism.n.01')
>>>Synset('living_thing.n.01')
>>>Synset('whole.n.02')
>>>Synset('object.n.01')
>>>Synset('physical_entity.n.01')
>>>Synset('entity.n.01')
```



### 传统的NLP：

- 通过同义词进行归纳
  - 由于同义词很多且较杂，无法准确的归纳出词义
- 使用向量表示词汇：
  - 某个位置为1其他0
  - 词越多向量越长
  - 无法表示任何词语**内在的联系** *（因为仅仅是不同下标位置1而已）*



### deep NLP

- 使用分布相似性[^1]归纳词义
  - 通过无数个包含目标词汇的句子，寻找与目标词汇相邻的单词来表达它的意思
  - *you shall know a word by the company it keeps*[^2]
- 每一个词由一组密集向量组成，用以预测单词所在文本的其他词汇
  - 这种表示方法被称为分布式表示（distributed representations）



> wt 指的是中心词汇
>
> w-t 指的是除了中心词汇以外的所有词汇



## Word2vec介绍

#### 目的：

- 选取一个词，并预测其**上下文出现词汇的概率**
- 每一个词的概率仅有一个，这个概率包括了上下文，而不是左边是什么右边是什么分别计算概率

#### 目标函数（loss function）：

![](https://latex.codecogs.com/gif.latex?J'(\theta )=\prod_{t=1}^{T} \prod_{-m\leq j\leq m}^{ }p(w_{t+j}\mid w_{t}i\theta ))

![](https://latex.codecogs.com/gif.latex?J(\theta )=-\frac{1}{T}\sum_{t=1}^{T} \sum_{-m\leq j\leq m}^{ } \lg 10(w_{t+j}\mid w_{t}))

> 注：这里使用log函数将求连乘转化为求连加，方便计算

#### 求概率分布：

![]( https://latex.codecogs.com/gif.latex?p(o\mid c) = \frac{exp(u_{o}^{T}v_{c})}{\sum_{w=1}^{v}exp(u_{w}^{T}v_{c})})

> - 传入参数c和o: 单词在词汇表空间中的索引及类型
>   - 带入t和t+j表示的就是单词在文本中的位置
>
> 

------




[^1]:  分布相似性（distributional similarity）：通过上下文来得到某个词的含义
[^2]:  通过单词的同伴来知道它的意义（JR Firth,英国语言学家）

